{"cells":[{"cell_type":"code","source":["import pyspark.sql.functions as f\n\n\n# This notebook contains all the function need for pyspark notebook \ndef get_kernel_for_x_y(index_x,index_y, data):\n        \"\"\"\n\n        Get a kernel with x,y as it's centre pixel.\n        Be aware that the x,y coordinates have to be in the same coordinate system as the coordinate system in the .tif file.\n        \n        This can be used if you want to use a matrix of pixels to predict the content of a centre pixel, which seems to be the standard in computer vision.\n        But this also leads to performance issue's since the number of pixel also increases with the matrix.\n\n        @param index_x: the x coordinate.\n        @param index_y: the y coordinate.\n        @return a kernel with chosen size in the init parameters\n        \"\"\"\n        \n        if sum([band[index_x][index_y] for band in data]) == 0:\n            return [0,0,0,0,0,0,0,0]\n        else:\n            spot_kernel = [band[index_x][index_y] for band in data]\n            spot_kernel.append(index_x)\n            spot_kernel.append(index_y)\n            spot_kernel = np.array(spot_kernel)\n            spot_kernel = spot_kernel.astype(int)\n            return spot_kernel\n          \nclass scaler_class_all:\n    \"\"\"\n    This class is used to scale columns from a raste file stored in a pandas dataframe to 0 and 1.\n\n    Scalers should have been made indepently!\n    \n    \"\"\"\n    def __init__(self, scaler_file_band1 = \"\", scaler_file_band2 = \"\", scaler_file_band3 = \"\", scaler_file_band4 = \"\", scaler_file_band5 = \"\", scaler_file_band6 = \"\") :\n        \"\"\"\n        Init of this class.\n\n        @param scaler_file_band1: Path to a file which contains the scaler for band 1.\n        @param scaler_file_band2: Path to a file which contains the scaler for band 2.\n        @param scaler_file_band3: Path to a file which contains the scaler for band 3.\n        @param scaler_file_band4: Path to a file which contains the scaler for band 4.\n        @param scaler_file_band5: Path to a file which contains the scaler for band 5.\n        @param scaler_file_band6: Path to a file which contains the scaler for band 6.    \n        \"\"\"\n\n        self.scaler_band1 = joblib.load(scaler_file_band1)\n        self.scaler_band2 = joblib.load(scaler_file_band2)\n        self.scaler_band3 = joblib.load(scaler_file_band3)\n        self.scaler_band4 = joblib.load(scaler_file_band4)\n        self.scaler_band5 = joblib.load(scaler_file_band5)\n        self.scaler_band6 = joblib.load(scaler_file_band6)\n\n\n    def transform(self,pixel_df, col_names = ['band1','band2','band3','band4','band5',\"band6\"]):\n        \"\"\"\n        Transforms the bands of a pandas dataframe.\n\n        @param pixel_df: dataframe in which bands column have to be scaled.\n        @return: dataframe with scaled bands.\n        \n        \"\"\"\n\n        pixel_df[col_names[0]] = self.scaler_band1.transform(pixel_df[col_names[0]].values.reshape(-1,1))        \n        pixel_df[col_names[1]] = self.scaler_band2.transform(pixel_df[col_names[1]].values.reshape(-1, 1))\n        pixel_df[col_names[2]] = self.scaler_band3.transform(pixel_df[col_names[2]].values.reshape(-1,1))        \n        pixel_df[col_names[3]] = self.scaler_band4.transform(pixel_df[col_names[3]].values.reshape(-1, 1))\n        pixel_df[col_names[4]] = self.scaler_band5.transform(pixel_df[col_names[4]].values.reshape(-1,1))        \n        pixel_df[col_names[5]] = self.scaler_band6.transform(pixel_df[col_names[5]].values.reshape(-1, 1))\n\n        return pixel_df\n\ndef func_cor_square(input_x_y):\n        \"\"\"\n        This function is used to make squares out of pixels for a inter connected output.\n        @param input_x_y a pixel input variable to be made into a square.\n        @return the the squared pixel.        \n        \"\"\"\n        rect = [round(input_x_y[0]/2)*2, round(input_x_y[1]/2)*2, 0, 0]\n        rect[2], rect[3] = rect[0] + 2, rect[1] + 2\n        coords = Polygon([(rect[0], rect[1]), (rect[2], rect[1]), (rect[2], rect[3]), (rect[0], rect[3]), (rect[0], rect[1])])\n        return coords\n\ndef func_cor_square_50cm(input_x_y):\n        \"\"\"\n        This function is used to make squares out of pixels for a inter connected output.\n        @param input_x_y a pixel input variable to be made into a square.\n        @return the the squared pixel.        \n        \"\"\"\n        rect = [input_x_y[0] - 0.5, input_x_y[1] - 0.5, 0, 0]\n        rect[2], rect[3] = rect[0] + 0.5, rect[1] + 0.5\n        coords = Polygon([(rect[0], rect[1]), (rect[2], rect[1]), (rect[2], rect[3]), (rect[0], rect[3]), (rect[0], rect[1])])\n        return coords      \n      \ndef dissolve_gpd_output(agpd, path_out):\n    \"\"\"\n    \n    This function is used to dissolve pixels to the same type of pixels thus decreasing the amount of space needed to save the file.\n    \n    @param agpd: a geopandas dataframe.\n    @param path_out: Location of where to store the dissolved output.\n    \"\"\"\n\n    dissolved = gpd.GeoDataFrame(columns=['label', 'geometry'], crs=agpd.crs)\n    labels = agpd['label'].unique()\n    #print(\"------\")\n    for label in labels:\n\n      #  print(label)\n        union_gpd = agpd[agpd['label'] == label].unary_union\n        dissolved = dissolved.append([{\"label\":label,\"geometry\":union_gpd}])\n    #print(\"------\")\n\n    if '.geojson' not in path_out:\n        dissolved.to_file(path_out) \n         \n    elif '.geojson' in path_out:\n        dissolved.to_file(path_out, driver=\"GeoJSON\")\n\ndef check_done_files():\n  dates = []\n  for file in glob.glob(path_to_output+\"*\"+model_path.split(\"/\")[-1].split(\".sav\")[0].split(\"_\")[-1].replace(\".\",\"_\")+\"*\"):\n    dates.append(file.split(\"/\")[-1].split(\"_\")[0])\n  return dates\n\n## pyspark udf functions\n\n# Pandas udf functions are at the moment the most fastest to do distributed predictions with.\n@f.pandas_udf('string')\ndef predict_pandas_udf(*cols):\n      \"\"\"\n      \n      Pyspark wrapper function for a sklearn model prediction.\n      \"\"\"\n      # cols will be a tuple of pandas.Series here.\n      X = pd.concat(cols, axis=1)\n      return pd.Series(loaded_model.predict(X))\n    \n#udf function for calculating mode, used for 2m aggregationds\n@f.udf\ndef mode(x):\n    from collections import Counter\n    return Counter(x).most_common(1)[0][0]\n  \n\n## main runner function  \ndef run_tif_model_implementer(a_path_to_tif_file, path_to_output, path_to_scalers, a_parts, a_model_path, aggregate_to_2m = True):\n  \"\"\"\n  \n  This function implements the actual loop which means that for every pixel it uses a model to predict a class.\n  The raster file can be split into multiple parts in order to reduce memory load.\n  \n  Normalization scalers \n  \n  @a_path_to_tif_file: Path to the raster file.\n  @a_parts: The number of parts to divide the dataset into reduce memory issue's\n  @a_model_path: Path to a model which needs to have a predict function with the input of the number of bands.\n  \"\"\"\n  # Set up parameters\n  path_to_tif_file = a_path_to_tif_file\n  parts = a_parts\n  model_path = a_model_path\n  loaded_model = pickle.load(open(model_path, 'rb'))\n  \n  start_run = timer()\n\n  # Bug still in databricks which does not let us directly write to azure blob\n  output_location_local = \"/home/\"+path_to_tif_file.split(\"/\")[-1].split(\".\")[0]+\".shp\"\n\n  use_kernels = False\n\n\n  output_location = path_to_output+path_to_tif_file.split(\"/\")[-1].split(\".\")[0]+\".shp\"\n\n  # Init the scaler to normalize values, we have to normalize values because of the differences between satellite images.\n  dataset = rasterio.open(path_to_tif_file)\n  meta = dataset.meta.copy()\n  data = dataset.read()\n  width, height = meta[\"width\"], meta[\"height\"]\n\n  # Determine which version of the AHN can be used \n  if int(path_to_tif_file.split(\"/\")[-1][0:4]) <= 2019:\n                  ahn_type = \"/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/scalers//ahn3.save\"\n  elif int(path_to_tif_file.split(\"/\")[-1][0:4]) > 2019:\n                  ahn_type = \"/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/scalers//ahn4.save\"\n  \n  # Load scalers for RGBI NDVI and Height, these scalers have to be premade.\n  a_normalize_scaler_class_all = scaler_class_all(scaler_file_band1 = glob.glob(path_to_scalers+path_to_tif_file.split(\"/\")[-1]+\"*band1*\")[0].replace(\"\\\\\",\"/\"), \\\n                                                      scaler_file_band2 = glob.glob(path_to_scalers+path_to_tif_file.split(\"/\")[-1]+\"*band2*\")[0].replace(\"\\\\\",\"/\"), \\\n                                                      scaler_file_band3 = glob.glob(path_to_scalers+path_to_tif_file.split(\"/\")[-1]+\"*band3*\")[0].replace(\"\\\\\",\"/\"), \\\n                                                      scaler_file_band4 = glob.glob(path_to_scalers+path_to_tif_file.split(\"/\")[-1]+\"*band4*\")[0].replace(\"\\\\\",\"/\"), \\\n                                                      scaler_file_band5 = glob.glob(path_to_scalers+path_to_tif_file.split(\"/\")[-1]+\"*band5*\")[0].replace(\"\\\\\",\"/\"), \\\n                                                      scaler_file_band6 = ahn_type)\n\n  # Declare variables to make divide the .tif file in different parts.\n  total_height = height\n\n  height_parts = round(total_height/parts)\n  begin_height = 0\n  end_height = height_parts\n\n  height_parts = total_height/parts\n\n  # Engage a loop which divides the pixels in parts.\n  for part in range(1,parts+1):\n\n    # Check if a precious already had certain parts done.\n    if os.path.isfile(output_location_local.replace(\".\",\"_part_\"+str(part)+\".\")) is True:\n      print(\"Part file already exists\")\n    else:\n    \n      permutations = list(itertools.product([x for x in range(begin_height, end_height)], [ y for y in range(0, width)]))\n      print(\"Total permutations this step: \"+str(len(permutations)))\n      # Calculate the number of permutations for this part.\n      start = timer()\n      if use_kernels is True:   \n          seg_df = [get_kernel_for_x_y(permutation[0],permutation[1],data) for permutation in permutations]\n          seg_df = pd.DataFrame(seg_df, columns= [\"r\",\"g\",\"b\",\"i\",\"ndvi\",\"height\",\"x\",\"y\"])\n      else:\n          seg_df = [band[begin_height:end_height].flatten() for band in data]\n          seg_df = pd.DataFrame(np.array(seg_df).T, columns= [\"r\",\"g\",\"b\",\"i\",\"ndvi\",\"height\"])\n          seg_df['x'] = [permutation[0] for permutation in permutations]\n          seg_df['y'] = [permutation[1] for permutation in permutations]\n      print(\"Done with extracting dataframe in \"+str(timer()-start)+\" second(s)\")\n\n      # Filter out empty pixels.\n      start = timer()\n      seg_df = seg_df[(seg_df[\"r\"] != 0) & (seg_df[\"g\"] != 0) & (seg_df[\"b\"] != 0) &  (seg_df[\"i\"] != 0)].reset_index().drop(['index'],axis=1)\n      seg_df['rd_x'],seg_df['rd_y'] = rasterio.transform.xy(dataset.transform, seg_df['x'], seg_df['y'])\n      print(\"Filtering done in \"+str(timer()-start)+\" second(s)\")\n      print(\"Filtered length of dataframe: \"+str(len(seg_df.index)))\n\n      # Scale/Normalize the pixels of this part\n      a_normalize_scaler_class_all.transform(seg_df, col_names=[\"r\",\"g\",\"b\",\"i\",\"ndvi\",\"height\"])\n      print(\"Normalization done in \"+str(timer()-start)+\" second(s)\")\n      \n      \n      # Create a Spark DataFrame and start predicting\n      start = timer()\n      sdf = spark.createDataFrame(seg_df.values.tolist(), [\"r\",\"g\",\"b\",\"i\",\"ndvi\",\"height\",\"x\",\"y\",\"rd_x\",\"rd_y\"])\n      list_of_columns = ['r', 'g', 'b', 'i', 'ndvi', 'height']\n      print(\"Finished making spark dataframe in \"+str(timer()-start)+\" second(s)\")\n\n      start = timer()\n      sdf = sdf.withColumn('label', predict_pandas_udf(*list_of_columns))\n\n      print(\"Predicting finished in: \"+str(timer()-start)+\" second(s)\")\n\n      # Aggregate to 2m for data reduction.\n      if aggregate_to_2m is True:\n\n        sdf = sdf.withColumn(\"group_x\",f.round(f.col(\"x\")/2)*2)\n        sdf = sdf.withColumn(\"group_y\",f.round(f.col(\"y\")/2)*2)\n        cols = ['label']\n        agg_expr = [mode(f.collect_list(col)).alias(col) for col in cols]\n        sdf.groupBy(['group_x','group_y']).agg(*agg_expr)\n\n        seg_df = sdf.toPandas()\n        print(\"Grouping to 2m finished in: \"+str(timer()-start)+\" second(s)\")\n\n      else:\n        seg_df = sdf.toPandas()\n        seg_df.columns = ['r', 'g', 'b', 'i', 'ndvi', 'height', 'x', 'y', 'rd_x', 'rd_y','label']\n        seg_df = seg_df[[\"rd_x\",\"rd_y\",\"label\"]]\n      \n      \n      if aggregate_to_2m is False:\n        start = timer()\n        #seg_df = gpd.GeoDataFrame(seg_df.groupby(\"label\").apply(lambda g: Polygon(gpd.points_from_xy(g['rd_x'],g['rd_y']))))\n        #seg_df['geometry'] = seg_df[0]\n        #seg_df = seg_df.drop([0],axis=1).reset_index()\n        seg_df['geometry'] = [func_cor_square_50cm(permutation) for permutation in seg_df[[\"rd_x\",\"rd_y\"] ].to_numpy().tolist()]\n        print(\"Grouping labels finished in: \"+str(timer()-start)+\" second(s)\")\n        \n      else: \n        seg_df['geometry'] = [func_cor_square(permutation) for permutation in seg_df[[\"rd_x\",\"rd_y\"] ].to_numpy().tolist()]\n\n      start = timer()\n      seg_df= seg_df[[\"geometry\",\"label\"]]\n\n      seg_df = gpd.GeoDataFrame(seg_df, geometry=seg_df.geometry)\n      seg_df = seg_df.set_crs(epsg = 28992)\n\n      dissolve_gpd_output(seg_df, output_location_local.replace(\".\",\"_part_\"+str(part)+\".\"))\n      print(\"Dissolving done in: \"+str(timer()-start)+\" second(s)\")\n      print(output_location_local.replace(\".\",\"_part_\"+str(part)+\".\"))\n\n\n    begin_height = int(round(end_height+1))\n    end_height = int(round(begin_height+height_parts))\n\n    if end_height > height - (1/2):\n         end_height = round(height - (1/2))\n        \n  # Merge the different parts into one.\n  first_check = 0\n  start = timer()\n\n  for file in glob.glob(output_location_local.replace(\".\",\"_part_*.\")):\n            print(file)\n\n            if first_check == 0:\n                 all_part = gpd.read_file(file)\n                 first_check = 1\n            else:\n                 print(\"Append\")\n                 all_part = all_part.append(gpd.read_file(file))\n                  \n               \n  all_part.dissolve(by='label').to_file(output_location_local)\n\n    \n  print(\"Done with merging files in: \"+str(timer()-start)+\" second(s)\")\n  for file in glob.glob(output_location_local.replace(\".\",\"_part_*.\").split(\".\")[0]):\n            os.remove(file)  \n\n  # Move the file from databricks vm to azure blob storage.\n  output_location_local = \"/home/\"+path_to_tif_file.split(\"/\")[-1].split(\".\")[0]+\".shp\"\n  output_location = path_to_output+path_to_tif_file.split(\"/\")[-1].split(\".\")[0]+\".shp\"\n  output_location = output_location.replace(\".shp\",\"_\"+model_path.split(\"/\")[-1]+\".shp\")\n  print(\"Writing to:\"+output_location)\n  shutil.move(output_location_local,output_location)\n  shutil.move(output_location_local.replace(\".shp\",\".cpg\"),output_location.replace(\".shp\",\".cpg\"))\n  shutil.move(output_location_local.replace(\".shp\",\".dbf\"),output_location.replace(\".shp\",\".dbf\"))\n  shutil.move(output_location_local.replace(\".shp\",\".shx\"),output_location.replace(\".shp\",\".shx\"))\n  print(\"Done with whole run in: \"+str(timer()-start_run)+\" second(s)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"955c2835-a4e0-4df7-a39d-66a87dbde792","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"tif_model_iterator_functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":206828270476791}},"nbformat":4,"nbformat_minor":0}
