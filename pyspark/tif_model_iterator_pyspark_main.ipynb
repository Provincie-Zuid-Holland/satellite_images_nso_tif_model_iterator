{"cells":[{"cell_type":"markdown","source":["# The TIF model iterator using pyspark\n\nThis notebook implements a tif iterator to use a sklearn model to predict every pixel in a raster .tif file.\nThis is done in a distributed way with pyspark because of the large number of pixels and time it will take without it.\n\nIt then outputs a segmentations shapefile which can be used in a GIS application to see the results.\n\nNote MinMaxScalers have to be made in order to predict because of the differences between images.\n\nAuthor: Michael de Winter"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"c61a2717-6627-4759-9380-ae4f1b82c257","inputWidgets":{},"title":"TIF model iterator"}}},{"cell_type":"code","source":["import rasterio\nimport pandas as pd\nimport itertools\nimport numpy as np\nimport glob\nimport joblib\nfrom pyspark.sql.functions import udf\nimport pickle\nfrom shapely.geometry import Polygon, Point\nfrom timeit import default_timer as timer\nimport geopandas as gpd\nimport os\nfrom pyspark.sql.types import DoubleType, StringType, ArrayType\nimport glob\nimport shutil"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6a485e58-c65a-4373-bfb6-989b6bec9582","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["###### run the below command to import all the required functions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c443219-06b2-45ba-9b92-2d8dd83e0b57","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run ./tif_model_iterator_functions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"208477fb-2595-4bb4-a50e-97265b49951c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["###Run settings"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0672bd15-3eda-4a7b-88ca-f068b1005a61","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Coepelduynen\npath_to_output = \"/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/\"\npath_to_scalers = \"/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/scalers/\"\nparts = 2\nmodel_path = \"/dbfs/mnt/satellite-images-nso/models/randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.3.sav\" "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8319118-abc8-4061-b5e6-5eba1833eaa1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["check_done_files()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f7124563-9410-47bb-b7f7-73b96f2e7ac0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[15]: [&#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190308&#39;,\n &#39;20190308&#39;,\n &#39;20190308&#39;,\n &#39;20190308&#39;,\n &#39;20190422&#39;,\n &#39;20190422&#39;,\n &#39;20190422&#39;,\n &#39;20190422&#39;,\n &#39;20190601&#39;,\n &#39;20190601&#39;,\n &#39;20190601&#39;,\n &#39;20190601&#39;,\n &#39;20191130&#39;,\n &#39;20191130&#39;,\n &#39;20191130&#39;,\n &#39;20191130&#39;,\n &#39;20191202&#39;,\n &#39;20191202&#39;,\n &#39;20191202&#39;,\n &#39;20191202&#39;,\n &#39;20200304&#39;,\n &#39;20200304&#39;,\n &#39;20200304&#39;,\n &#39;20200304&#39;,\n &#39;20200323&#39;,\n &#39;20200323&#39;,\n &#39;20200323&#39;,\n &#39;20200323&#39;,\n &#39;20200326&#39;,\n &#39;20200326&#39;,\n &#39;20200326&#39;,\n &#39;20200326&#39;,\n &#39;20200508&#39;,\n &#39;20200508&#39;,\n &#39;20200508&#39;,\n &#39;20200508&#39;,\n &#39;20200625&#39;,\n &#39;20200625&#39;,\n &#39;20200625&#39;,\n &#39;20200625&#39;,\n &#39;20200731&#39;,\n &#39;20200731&#39;,\n &#39;20200731&#39;,\n &#39;20200731&#39;,\n &#39;20200915&#39;,\n &#39;20200915&#39;,\n &#39;20200915&#39;,\n &#39;20200915&#39;,\n &#39;20201231&#39;,\n &#39;20201231&#39;,\n &#39;20201231&#39;,\n &#39;20201231&#39;,\n &#39;20210302&#39;,\n &#39;20210302&#39;,\n &#39;20210302&#39;,\n &#39;20210302&#39;,\n &#39;20210423&#39;,\n &#39;20210423&#39;,\n &#39;20210423&#39;,\n &#39;20210423&#39;,\n &#39;20210709&#39;,\n &#39;20210709&#39;,\n &#39;20210709&#39;,\n &#39;20210709&#39;,\n &#39;20210815&#39;,\n &#39;20210815&#39;,\n &#39;20210815&#39;,\n &#39;20210815&#39;,\n &#39;20210907&#39;,\n &#39;20210907&#39;,\n &#39;20210907&#39;,\n &#39;20210907&#39;,\n &#39;20220515&#39;,\n &#39;20220515&#39;,\n &#39;20220515&#39;,\n &#39;20220515&#39;,\n &#39;20220922&#39;,\n &#39;20220922&#39;,\n &#39;20220922&#39;,\n &#39;20220922&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[15]: [&#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190302&#39;,\n &#39;20190308&#39;,\n &#39;20190308&#39;,\n &#39;20190308&#39;,\n &#39;20190308&#39;,\n &#39;20190422&#39;,\n &#39;20190422&#39;,\n &#39;20190422&#39;,\n &#39;20190422&#39;,\n &#39;20190601&#39;,\n &#39;20190601&#39;,\n &#39;20190601&#39;,\n &#39;20190601&#39;,\n &#39;20191130&#39;,\n &#39;20191130&#39;,\n &#39;20191130&#39;,\n &#39;20191130&#39;,\n &#39;20191202&#39;,\n &#39;20191202&#39;,\n &#39;20191202&#39;,\n &#39;20191202&#39;,\n &#39;20200304&#39;,\n &#39;20200304&#39;,\n &#39;20200304&#39;,\n &#39;20200304&#39;,\n &#39;20200323&#39;,\n &#39;20200323&#39;,\n &#39;20200323&#39;,\n &#39;20200323&#39;,\n &#39;20200326&#39;,\n &#39;20200326&#39;,\n &#39;20200326&#39;,\n &#39;20200326&#39;,\n &#39;20200508&#39;,\n &#39;20200508&#39;,\n &#39;20200508&#39;,\n &#39;20200508&#39;,\n &#39;20200625&#39;,\n &#39;20200625&#39;,\n &#39;20200625&#39;,\n &#39;20200625&#39;,\n &#39;20200731&#39;,\n &#39;20200731&#39;,\n &#39;20200731&#39;,\n &#39;20200731&#39;,\n &#39;20200915&#39;,\n &#39;20200915&#39;,\n &#39;20200915&#39;,\n &#39;20200915&#39;,\n &#39;20201231&#39;,\n &#39;20201231&#39;,\n &#39;20201231&#39;,\n &#39;20201231&#39;,\n &#39;20210302&#39;,\n &#39;20210302&#39;,\n &#39;20210302&#39;,\n &#39;20210302&#39;,\n &#39;20210423&#39;,\n &#39;20210423&#39;,\n &#39;20210423&#39;,\n &#39;20210423&#39;,\n &#39;20210709&#39;,\n &#39;20210709&#39;,\n &#39;20210709&#39;,\n &#39;20210709&#39;,\n &#39;20210815&#39;,\n &#39;20210815&#39;,\n &#39;20210815&#39;,\n &#39;20210815&#39;,\n &#39;20210907&#39;,\n &#39;20210907&#39;,\n &#39;20210907&#39;,\n &#39;20210907&#39;,\n &#39;20220515&#39;,\n &#39;20220515&#39;,\n &#39;20220515&#39;,\n &#39;20220515&#39;,\n &#39;20220922&#39;,\n &#39;20220922&#39;,\n &#39;20220922&#39;,\n &#39;20220922&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Coepelduynen"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"26d2ad14-0961-44df-808c-91aa3bbf100c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["for file  in glob.glob(\"/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/*.tif\"):\n  print(file.split(\"/\")[-1].split(\"_\")[0])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"08bc579a-4518-454e-b934-8dc9b91c1b39","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">20190302\n20190302\n20190308\n20190422\n20190601\n20191130\n20191202\n20200304\n20200323\n20200326\n20200508\n20200625\n20200731\n20200915\n20201231\n20210302\n20210423\n20210709\n20210815\n20210907\n20211226\n20220302\n20220501\n20220514\n20220515\n20220816\n20220922\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">20190302\n20190302\n20190308\n20190422\n20190601\n20191130\n20191202\n20200304\n20200323\n20200326\n20200508\n20200625\n20200731\n20200915\n20201231\n20210302\n20210423\n20210709\n20210815\n20210907\n20211226\n20220302\n20220501\n20220514\n20220515\n20220816\n20220922\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Predict all \nfor file  in glob.glob(\"/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/*.tif\"):\n  if file.split(\"/\")[-1].split(\"_\")[0] not in set(check_done_files()):\n    try:\n      print(file)\n      run_tif_model_implementer(file,path_to_output,path_to_scalers,parts,model_path, aggregate_to_2m = False)\n    except Exception as e:\n      print(e)\n  else:\n      print(file.split(\"/\")[-1].split(\"_\")[0]+\" already done\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ef07dd1-51dc-494c-bbc5-34ed75988207","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">20190302 already done\n20190302 already done\n20190308 already done\n20190422 already done\n20190601 already done\n20191130 already done\n20191202 already done\n20200304 already done\n20200323 already done\n20200326 already done\n20200508 already done\n20200625 already done\n20200731 already done\n20200915 already done\n20201231 already done\n20210302 already done\n20210423 already done\n20210709 already done\n20210815 already done\n20210907 already done\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20211226_103526_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 8.513373872000102 second(s)\nFiltering done in 38.381067487999644 second(s)\nFiltered length of dataframe: 3535439\nNormalization done in 38.55129070999965 second(s)\nFinished making spark dataframe in 195.596199695 second(s)\nPredicting finished in: 0.13339372999962507 second(s)\n/databricks/spark/python/pyspark/sql/pandas/conversion.py:161: UserWarning: toPandas attempted Arrow optimization because &#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has reached the error below and can not continue. Note that &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; does not have an effect on failures in the middle of computation.\n  An error occurred while calling o1335.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 0.0 failed 4 times, most recent failure: Lost task 20.3 in stage 0.0 (TID 55) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n  warnings.warn(msg)\nAn error occurred while calling o1335.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 0.0 failed 4 times, most recent failure: Lost task 20.3 in stage 0.0 (TID 55) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\n*** WARNING: skipped 88767 bytes of output ***\n\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n  warnings.warn(msg)\nAn error occurred while calling o3634.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 4.0 failed 4 times, most recent failure: Lost task 5.3 in stage 4.0 (TID 357) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n20220922 already done\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">20190302 already done\n20190302 already done\n20190308 already done\n20190422 already done\n20190601 already done\n20191130 already done\n20191202 already done\n20200304 already done\n20200323 already done\n20200326 already done\n20200508 already done\n20200625 already done\n20200731 already done\n20200915 already done\n20201231 already done\n20210302 already done\n20210423 already done\n20210709 already done\n20210815 already done\n20210907 already done\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20211226_103526_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 8.513373872000102 second(s)\nFiltering done in 38.381067487999644 second(s)\nFiltered length of dataframe: 3535439\nNormalization done in 38.55129070999965 second(s)\nFinished making spark dataframe in 195.596199695 second(s)\nPredicting finished in: 0.13339372999962507 second(s)\n/databricks/spark/python/pyspark/sql/pandas/conversion.py:161: UserWarning: toPandas attempted Arrow optimization because &#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has reached the error below and can not continue. Note that &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; does not have an effect on failures in the middle of computation.\n  An error occurred while calling o1335.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 0.0 failed 4 times, most recent failure: Lost task 20.3 in stage 0.0 (TID 55) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n  warnings.warn(msg)\nAn error occurred while calling o1335.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 0.0 failed 4 times, most recent failure: Lost task 20.3 in stage 0.0 (TID 55) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\n*** WARNING: skipped 88767 bytes of output ***\n\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n  warnings.warn(msg)\nAn error occurred while calling o3634.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 4.0 failed 4 times, most recent failure: Lost task 5.3 in stage 4.0 (TID 357) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n20220922 already done\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["date_annotations = [20190601, 20200625, 20200731, 20200915, 20210709, 20210815,\n       20210907, 20220515, 20220922]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"52fa65bf-6cfc-4a5f-a356-fe040a3aaa6c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["check_done_files()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a63de94-ca4e-4329-9157-a644915027a3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[18]: []</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: []</div>"]}}],"execution_count":0},{"cell_type":"code","source":["for date in date_annotations:\n  print(date)\n  \n  file = glob.glob(\"/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/\"+str(date)+\"*.tif\")[0]\n  if file.split(\"/\")[-1].split(\"_\")[0] not in set(check_done_files()):\n    print(\"File not done\")\n    run_tif_model_implementer(file,path_to_output,path_to_scalers,parts,model_path, aggregate_to_2m = False)\n  else:\n    print(\"File done\")\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7c33c8d8-ff20-4b66-8f0a-4d15aef15399","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">20190601\nFile done\n20200625\nFile done\n20200731\nFile done\n20200915\nFile done\n20210709\nFile done\n20210815\nFile done\n20210907\nFile done\n20220515\nFile not done\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.698784083999954 second(s)\nFiltering done in 40.1953813990001 second(s)\nFiltered length of dataframe: 3552354\nNormalization done in 40.3807953160001 second(s)\nFinished making spark dataframe in 183.56916892599997 second(s)\nPredicting finished in: 0.5248957400001473 second(s)\nGrouping labels finished in: 46.83219905200008 second(s)\nDissolving done in: 403.8340583239999 second(s)\n/home/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.09839975399973 second(s)\nFiltering done in 44.77717183300001 second(s)\nFiltered length of dataframe: 3979837\nNormalization done in 44.92628003799973 second(s)\nFinished making spark dataframe in 198.99546538199957 second(s)\nPredicting finished in: 0.04212436699981481 second(s)\nGrouping labels finished in: 50.01575391999995 second(s)\nDissolving done in: 421.4746051070001 second(s)\n/home/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\nDone with merging files in: 903.0280544790003 second(s)\nWriting to:/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.3.sav.shp\nDone with whole run in: 2366.013574098 second(s)\n20220922\nFile not done\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.077861441000096 second(s)\nFiltering done in 40.10247311399962 second(s)\nFiltered length of dataframe: 3553522\nNormalization done in 40.237125320999894 second(s)\nFinished making spark dataframe in 178.67396375199996 second(s)\nPredicting finished in: 0.34041518899994117 second(s)\nGrouping labels finished in: 45.67730325599996 second(s)\nDissolving done in: 785.9484591440005 second(s)\n/home/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nTotal permutations this step: 15411162\nDone with extracting dataframe in 21.516289228999995 second(s)\nFiltering done in 44.41414065199933 second(s)\nFiltered length of dataframe: 3979837\nNormalization done in 44.55456310499994 second(s)\nFinished making spark dataframe in 199.5694999819998 second(s)\nPredicting finished in: 0.03834587599976658 second(s)\nGrouping labels finished in: 53.04835336399992 second(s)\nDissolving done in: 474.0991576640008 second(s)\n/home/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\n/home/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nAppend\nDone with merging files in: 1715.719163353 second(s)\nWriting to:/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.3.sav.shp\nDone with whole run in: 3586.792346212 second(s)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">20190601\nFile done\n20200625\nFile done\n20200731\nFile done\n20200915\nFile done\n20210709\nFile done\n20210815\nFile done\n20210907\nFile done\n20220515\nFile not done\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.698784083999954 second(s)\nFiltering done in 40.1953813990001 second(s)\nFiltered length of dataframe: 3552354\nNormalization done in 40.3807953160001 second(s)\nFinished making spark dataframe in 183.56916892599997 second(s)\nPredicting finished in: 0.5248957400001473 second(s)\nGrouping labels finished in: 46.83219905200008 second(s)\nDissolving done in: 403.8340583239999 second(s)\n/home/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.09839975399973 second(s)\nFiltering done in 44.77717183300001 second(s)\nFiltered length of dataframe: 3979837\nNormalization done in 44.92628003799973 second(s)\nFinished making spark dataframe in 198.99546538199957 second(s)\nPredicting finished in: 0.04212436699981481 second(s)\nGrouping labels finished in: 50.01575391999995 second(s)\nDissolving done in: 421.4746051070001 second(s)\n/home/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\nDone with merging files in: 903.0280544790003 second(s)\nWriting to:/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.3.sav.shp\nDone with whole run in: 2366.013574098 second(s)\n20220922\nFile not done\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.077861441000096 second(s)\nFiltering done in 40.10247311399962 second(s)\nFiltered length of dataframe: 3553522\nNormalization done in 40.237125320999894 second(s)\nFinished making spark dataframe in 178.67396375199996 second(s)\nPredicting finished in: 0.34041518899994117 second(s)\nGrouping labels finished in: 45.67730325599996 second(s)\nDissolving done in: 785.9484591440005 second(s)\n/home/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nTotal permutations this step: 15411162\nDone with extracting dataframe in 21.516289228999995 second(s)\nFiltering done in 44.41414065199933 second(s)\nFiltered length of dataframe: 3979837\nNormalization done in 44.55456310499994 second(s)\nFinished making spark dataframe in 199.5694999819998 second(s)\nPredicting finished in: 0.03834587599976658 second(s)\nGrouping labels finished in: 53.04835336399992 second(s)\nDissolving done in: 474.0991576640008 second(s)\n/home/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\n/home/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nAppend\nDone with merging files in: 1715.719163353 second(s)\nWriting to:/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.3.sav.shp\nDone with whole run in: 3586.792346212 second(s)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["for file  in glob.glob(\"/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/*\"):\n  print(file)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43411c84-2e87-419d-bd4b-87c68fdd380b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200304_114601_SV1-02_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200323_112115_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200326_114323_SV1-02_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200508_110812_SV1-01_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20201231_105943_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210302_111247_SV1-03_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210423_104948_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210709_103835_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20211226_103526_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220302_112108_SV1-04_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220501_111418_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220514_114854_SV1-02_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220816_111150_SV2-01_SV_RD_11bit_RGBI_50cm_Scheveningen_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/scalers\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200304_114601_SV1-02_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200323_112115_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200326_114323_SV1-02_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200508_110812_SV1-01_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20201231_105943_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210302_111247_SV1-03_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210423_104948_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210709_103835_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20211226_103526_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220302_112108_SV1-04_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220501_111418_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220514_114854_SV1-02_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220816_111150_SV2-01_SV_RD_11bit_RGBI_50cm_Scheveningen_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220922_110546_SV2-01_SV_RD_11bit_RGBI_50cm_Voorhout_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/scalers\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sh\n\nls /dbfs/mnt/annotations/coepelduynen"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a5a6be21-678e-48f0-9ac5-11b7cc8005df","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Annotaties Coepelduynen 20190601.gpkg\nAnnotaties Coepelduynen 20200625.gpkg\nAnnotaties Coepelduynen 20200731.gpkg\nAnnotaties Coepelduynen 20200731.gpkg-shm\nAnnotaties Coepelduynen 20200731.gpkg-wal\nAnnotaties Coepelduynen 20200915.gpkg\nAnnotaties Coepelduynen 20200915.gpkg-shm\nAnnotaties Coepelduynen 20200915.gpkg-wal\nAnnotaties Coepelduynen 20210709.gpkg\nAnnotaties Coepelduynen 20210815.gpkg\nAnnotaties Coepelduynen 20210907.gpkg\nAnnotaties Coepelduynen 20220515.gpkg\nAnnotaties Coepelduynen 20220922.gpkg\nannotaties_coepelduynen_to_pixel.csv\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Annotaties Coepelduynen 20190601.gpkg\nAnnotaties Coepelduynen 20200625.gpkg\nAnnotaties Coepelduynen 20200731.gpkg\nAnnotaties Coepelduynen 20200731.gpkg-shm\nAnnotaties Coepelduynen 20200731.gpkg-wal\nAnnotaties Coepelduynen 20200915.gpkg\nAnnotaties Coepelduynen 20200915.gpkg-shm\nAnnotaties Coepelduynen 20200915.gpkg-wal\nAnnotaties Coepelduynen 20210709.gpkg\nAnnotaties Coepelduynen 20210815.gpkg\nAnnotaties Coepelduynen 20210907.gpkg\nAnnotaties Coepelduynen 20220515.gpkg\nAnnotaties Coepelduynen 20220922.gpkg\nannotaties_coepelduynen_to_pixel.csv\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dates_annotations = [file.split(\" \")[-1].split(\".\")[0] for file in glob.glob(\"/dbfs/mnt/annotations/coepelduynen/*.gpkg\")]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80dbef6b-9dc6-4ece-b93f-52611ee752bf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dates_annotations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ba0e9375-1ce5-41ae-836a-8d4c9bdeb9b3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[6]: [&#39;20190601&#39;,\n &#39;20200625&#39;,\n &#39;20200731&#39;,\n &#39;20200915&#39;,\n &#39;20210709&#39;,\n &#39;20210815&#39;,\n &#39;20210907&#39;,\n &#39;20220515&#39;,\n &#39;20220922&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: [&#39;20190601&#39;,\n &#39;20200625&#39;,\n &#39;20200731&#39;,\n &#39;20200915&#39;,\n &#39;20210709&#39;,\n &#39;20210815&#39;,\n &#39;20210907&#39;,\n &#39;20220515&#39;,\n &#39;20220922&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dates_annotations.append(\"20200508\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c97c48f9-e6fa-4409-b529-90b8886b4370","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dates_annotations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"13f9848d-8d33-451f-ae5f-d89fecb82e78","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[11]: [&#39;20190601&#39;,\n &#39;20200625&#39;,\n &#39;20200731&#39;,\n &#39;20200915&#39;,\n &#39;20210815&#39;,\n &#39;20210907&#39;,\n &#39;20220515&#39;,\n &#39;20220922&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: [&#39;20190601&#39;,\n &#39;20200625&#39;,\n &#39;20200731&#39;,\n &#39;20200915&#39;,\n &#39;20210815&#39;,\n &#39;20210907&#39;,\n &#39;20220515&#39;,\n &#39;20220922&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# For the annotations.\nfor date in dates_annotations:\n  path_to_tif_file = glob.glob('/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/'+date+'*')[0]\n  print(path_to_tif_file)\n  run_tif_model_implementer(path_to_tif_file,path_to_output,path_to_scalers,parts,model_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fb71deee-b018-4b45-ac9c-cd09857bff50","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.956320405000042 second(s)\nFiltering done in 39.360326378000536 second(s)\nNormalization done in 39.549338319000526 second(s)\nFinished making spark dataframe in 190.2881780560001 second(s)\nPredicting finished in: 0.36836192200007645 second(s)\n                r         g         b  ...  label  group_x  group_y\n0        0.265449  0.197120  0.163441  ...    Bos      0.0   4802.0\n1        0.263837  0.192169  0.160215  ...    Bos      0.0   4802.0\n2        0.265449  0.197120  0.163441  ...    Bos      2.0   4802.0\n3        0.263837  0.192169  0.160215  ...    Bos      2.0   4802.0\n4        0.268135  0.202520  0.169355  ...    Bos      2.0   4798.0\n...           ...       ...       ...  ...    ...      ...      ...\n3551971  0.258463  0.203420  0.162903  ...   Gras   3198.0   2940.0\n3551972  0.254702  0.198920  0.156989  ...   Gras   3198.0   2940.0\n3551973  0.252552  0.195770  0.152688  ...   Gras   3198.0   2942.0\n3551974  0.242343  0.184068  0.137634  ...    Bos   3198.0   2942.0\n3551975  0.230521  0.171017  0.121505  ...    Bos   3198.0   2944.0\n\n[3551976 rows x 13 columns]\nGrouping to 2m finished in: 7.218818955000643 second(s)\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 19.365496979000454 second(s)\nFiltering done in 44.12743587600016 second(s)\nNormalization done in 44.26805343200067 second(s)\nFinished making spark dataframe in 204.66359082000054 second(s)\nPredicting finished in: 0.04718929999944521 second(s)\n                r         g         b  ...  label  group_x  group_y\n0        0.322945  0.273177  0.265591  ...   Gras   3200.0   1398.0\n1        0.296077  0.242574  0.227419  ...   Gras   3200.0   1400.0\n2        0.279957  0.224572  0.204839  ...   Gras   3200.0   1400.0\n3        0.292853  0.238074  0.222581  ...   Gras   3200.0   1402.0\n4        0.311123  0.257876  0.248387  ...   Gras   3200.0   1402.0\n...           ...       ...       ...  ...    ...      ...      ...\n3981376  0.311123  0.252025  0.247312  ...   Gras   6398.0    912.0\n3981377  0.311123  0.252925  0.248387  ...   Gras   6398.0    912.0\n3981378  0.315422  0.258326  0.254839  ...   Gras   6398.0    914.0\n3981379  0.319183  0.262376  0.259677  ...   Gras   6398.0    914.0\n3981380  0.318109  0.260576  0.257527  ...   Gras   6398.0    916.0\n\n[3981381 rows x 13 columns]\nGrouping to 2m finished in: 6.974870798999291 second(s)\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\nDone with merging files in: 24.639111288000095 second(s)\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 8.51354127400009 second(s)\nFiltering done in 39.0579912660005 second(s)\nNormalization done in 39.17698162800025 second(s)\nFinished making spark dataframe in 182.31897571900026 second(s)\nPredicting finished in: 0.3539856070001406 second(s)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.956320405000042 second(s)\nFiltering done in 39.360326378000536 second(s)\nNormalization done in 39.549338319000526 second(s)\nFinished making spark dataframe in 190.2881780560001 second(s)\nPredicting finished in: 0.36836192200007645 second(s)\n                r         g         b  ...  label  group_x  group_y\n0        0.265449  0.197120  0.163441  ...    Bos      0.0   4802.0\n1        0.263837  0.192169  0.160215  ...    Bos      0.0   4802.0\n2        0.265449  0.197120  0.163441  ...    Bos      2.0   4802.0\n3        0.263837  0.192169  0.160215  ...    Bos      2.0   4802.0\n4        0.268135  0.202520  0.169355  ...    Bos      2.0   4798.0\n...           ...       ...       ...  ...    ...      ...      ...\n3551971  0.258463  0.203420  0.162903  ...   Gras   3198.0   2940.0\n3551972  0.254702  0.198920  0.156989  ...   Gras   3198.0   2940.0\n3551973  0.252552  0.195770  0.152688  ...   Gras   3198.0   2942.0\n3551974  0.242343  0.184068  0.137634  ...    Bos   3198.0   2942.0\n3551975  0.230521  0.171017  0.121505  ...    Bos   3198.0   2944.0\n\n[3551976 rows x 13 columns]\nGrouping to 2m finished in: 7.218818955000643 second(s)\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 19.365496979000454 second(s)\nFiltering done in 44.12743587600016 second(s)\nNormalization done in 44.26805343200067 second(s)\nFinished making spark dataframe in 204.66359082000054 second(s)\nPredicting finished in: 0.04718929999944521 second(s)\n                r         g         b  ...  label  group_x  group_y\n0        0.322945  0.273177  0.265591  ...   Gras   3200.0   1398.0\n1        0.296077  0.242574  0.227419  ...   Gras   3200.0   1400.0\n2        0.279957  0.224572  0.204839  ...   Gras   3200.0   1400.0\n3        0.292853  0.238074  0.222581  ...   Gras   3200.0   1402.0\n4        0.311123  0.257876  0.248387  ...   Gras   3200.0   1402.0\n...           ...       ...       ...  ...    ...      ...      ...\n3981376  0.311123  0.252025  0.247312  ...   Gras   6398.0    912.0\n3981377  0.311123  0.252925  0.248387  ...   Gras   6398.0    912.0\n3981378  0.315422  0.258326  0.254839  ...   Gras   6398.0    914.0\n3981379  0.319183  0.262376  0.259677  ...   Gras   6398.0    914.0\n3981380  0.318109  0.260576  0.257527  ...   Gras   6398.0    916.0\n\n[3981381 rows x 13 columns]\nGrouping to 2m finished in: 6.974870798999291 second(s)\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\nDone with merging files in: 24.639111288000095 second(s)\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 8.51354127400009 second(s)\nFiltering done in 39.0579912660005 second(s)\nNormalization done in 39.17698162800025 second(s)\nFinished making spark dataframe in 182.31897571900026 second(s)\nPredicting finished in: 0.3539856070001406 second(s)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["for date in dates_annotations:\n  path_to_tif_file = glob.glob('/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/'+date+'*')[0]\n  print(path_to_tif_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de7a21d7-54d8-4c46-b3b4-09461d190d49","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IndexError</span>                                Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-741312215532814&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">for</span> date <span class=\"ansi-green-fg\">in</span> dates_annotations<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\">   </span>path_to_tif_file <span class=\"ansi-blue-fg\">=</span> glob<span class=\"ansi-blue-fg\">.</span>glob<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/&#39;</span><span class=\"ansi-blue-fg\">+</span>date<span class=\"ansi-blue-fg\">+</span><span class=\"ansi-blue-fg\">&#39;*&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>   print<span class=\"ansi-blue-fg\">(</span>path_to_tif_file<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">IndexError</span>: list index out of range</div>","errorSummary":"<span class=\"ansi-red-fg\">IndexError</span>: list index out of range","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IndexError</span>                                Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-741312215532814&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">for</span> date <span class=\"ansi-green-fg\">in</span> dates_annotations<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\">   </span>path_to_tif_file <span class=\"ansi-blue-fg\">=</span> glob<span class=\"ansi-blue-fg\">.</span>glob<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/&#39;</span><span class=\"ansi-blue-fg\">+</span>date<span class=\"ansi-blue-fg\">+</span><span class=\"ansi-blue-fg\">&#39;*&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>   print<span class=\"ansi-blue-fg\">(</span>path_to_tif_file<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">IndexError</span>: list index out of range</div>"]}}],"execution_count":0},{"cell_type":"code","source":["run_tif_model_implementer(\"/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\",path_to_output,path_to_scalers,parts,model_path, aggregate_to_2m = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"86173916-09b1-45e7-a9ca-5266b6289973","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 8.687650267999743 second(s)\nFiltering done in 38.01366129300004 second(s)\nFiltered length of dataframe: 3553522\nNormalization done in 38.125182054000106 second(s)\nFinished making spark dataframe in 183.10271757300052 second(s)\nPredicting finished in: 0.32176552700002503 second(s)\n             rd_x       rd_y     label\n0        89908.75  472598.75       Bos\n1        89909.25  472598.75       Bos\n2        89906.75  472598.25       Bos\n3        89907.25  472598.25      Gras\n4        89907.75  472598.25       Bos\n...           ...        ...       ...\n3553517  88977.75  470999.75  Struweel\n3553518  88978.25  470999.75       Bos\n3553519  88978.75  470999.75       Bos\n3553520  88979.25  470999.75       Bos\n3553521  88979.75  470999.75       Bos\n\n[3553522 rows x 3 columns]\nGrouping labels finished in: 52.716670160999456 second(s)\n/home/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 19.84464542600108 second(s)\nFiltering done in 41.987040443000296 second(s)\nFiltered length of dataframe: 3979837\nNormalization done in 42.10999645300035 second(s)\nFinished making spark dataframe in 203.10164826500113 second(s)\nPredicting finished in: 0.043726697000238346 second(s)\n             rd_x       rd_y label\n0        88206.75  470998.75  Gras\n1        88207.25  470998.75  Gras\n2        88207.75  470998.75  Gras\n3        88208.25  470998.75  Gras\n4        88208.75  470998.75  Gras\n...           ...        ...   ...\n3979832  87964.25  469400.75  Gras\n3979833  87964.75  469400.75  Gras\n3979834  87965.25  469400.75  Gras\n3979835  87965.75  469400.75  Gras\n3979836  87965.75  469400.25  Gras\n\n[3979837 rows x 3 columns]\nGrouping labels finished in: 56.88511645699873 second(s)\n/home/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\n/home/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nAppend\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 8.687650267999743 second(s)\nFiltering done in 38.01366129300004 second(s)\nFiltered length of dataframe: 3553522\nNormalization done in 38.125182054000106 second(s)\nFinished making spark dataframe in 183.10271757300052 second(s)\nPredicting finished in: 0.32176552700002503 second(s)\n             rd_x       rd_y     label\n0        89908.75  472598.75       Bos\n1        89909.25  472598.75       Bos\n2        89906.75  472598.25       Bos\n3        89907.25  472598.25      Gras\n4        89907.75  472598.25       Bos\n...           ...        ...       ...\n3553517  88977.75  470999.75  Struweel\n3553518  88978.25  470999.75       Bos\n3553519  88978.75  470999.75       Bos\n3553520  88979.25  470999.75       Bos\n3553521  88979.75  470999.75       Bos\n\n[3553522 rows x 3 columns]\nGrouping labels finished in: 52.716670160999456 second(s)\n/home/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 19.84464542600108 second(s)\nFiltering done in 41.987040443000296 second(s)\nFiltered length of dataframe: 3979837\nNormalization done in 42.10999645300035 second(s)\nFinished making spark dataframe in 203.10164826500113 second(s)\nPredicting finished in: 0.043726697000238346 second(s)\n             rd_x       rd_y label\n0        88206.75  470998.75  Gras\n1        88207.25  470998.75  Gras\n2        88207.75  470998.75  Gras\n3        88208.25  470998.75  Gras\n4        88208.75  470998.75  Gras\n...           ...        ...   ...\n3979832  87964.25  469400.75  Gras\n3979833  87964.75  469400.75  Gras\n3979834  87965.25  469400.75  Gras\n3979835  87965.75  469400.75  Gras\n3979836  87965.75  469400.25  Gras\n\n[3979837 rows x 3 columns]\nGrouping labels finished in: 56.88511645699873 second(s)\n/home/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\n/home/20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nAppend\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import os\n\nfor file in glob.glob(\"/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/*v1.2*\"):\n  print(file)\n  os.rename(file,file.split(\"_coepelduynen_contrast_annotations_\")[0]+file.split(\"_coepelduynen_contrast_annotations_\")[-1].split(\".sav\")[-1])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5d2624b2-2eb0-44c0-bdf8-b125e8077613","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.cpg\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.dbf\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.shp\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.shx\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.cpg\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.dbf\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.shp\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.shx\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.cpg\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.dbf\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.shp\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.shx\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.cpg\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.dbf\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.shp\n/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small_balanced_v1.2.sav.shx\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["for file in glob.glob(\"/dbfs/mnt/satellite-images-nso/model_out_coepelduynen/*v1.2*\"):\n  print(file)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0be51b33-ca21-4065-9352-5e4542130051","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# All files.\nfor path_to_tif_file in glob.glob(\"/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/*\"):\n  print(path_to_tif_file)\n  run_tif_model_implementer(path_to_tif_file,path_to_output,path_to_scalers,parts,model_path, aggregate_to_2m= False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0cf8a087-2ab8-48a2-9942-a26090a3540f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 10.427086648000113 second(s)\nFiltering done in 40.352837850998185 second(s)\nNormalization done in 40.53532803900089 second(s)\nFinished making spark dataframe in 189.97698182399836 second(s)\nPredicting finished in: 6.262001445997157 second(s)\nGrouping for output\n/home/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.480283937999047 second(s)\nFiltering done in 46.32159833699916 second(s)\nNormalization done in 46.45904918800079 second(s)\nFinished making spark dataframe in 208.53914567399988 second(s)\nPredicting finished in: 6.2967404990013165 second(s)\nGrouping for output\n/home/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.251330809001956 second(s)\nFiltering done in 41.017747896999936 second(s)\nNormalization done in 41.14325455500148 second(s)\nFinished making spark dataframe in 187.6864122900006 second(s)\nPredicting finished in: 6.733306923000782 second(s)\nGrouping for output\n/home/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.885407263998786 second(s)\nFiltering done in 44.833345695999014 second(s)\nNormalization done in 44.97876207399895 second(s)\nFinished making spark dataframe in 208.53120297499845 second(s)\nPredicting finished in: 6.176030564998655 second(s)\nGrouping for output\n/home/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\n/home/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.243611224999768 second(s)\nFiltering done in 39.65820740599884 second(s)\nNormalization done in 39.78284267499839 second(s)\nFinished making spark dataframe in 187.09885762900012 second(s)\nPredicting finished in: 6.456363535999117 second(s)\nGrouping for output\n/home/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 21.586395120000816 second(s)\nFiltering done in 46.69168740599707 second(s)\nNormalization done in 46.88244692399894 second(s)\nFinished making spark dataframe in 219.9449768020022 second(s)\nPredicting finished in: 5.240294942999753 second(s)\nGrouping for output\n/home/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\n/home/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.12125247899894 second(s)\nFiltering done in 40.73236160599845 second(s)\nNormalization done in 40.87371607000023 second(s)\nFinished making spark dataframe in 185.08361921899996 second(s)\nPredicting finished in: 5.65575145099865 second(s)\nGrouping for output\n/home/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.866702085000725 second(s)\nFiltering done in 43.94590059499751 second(s)\nNormalization done in 44.13988108400008 second(s)\nFinished making spark dataframe in 208.1865519239982 second(s)\nPredicting finished in: 5.21642539000095 second(s)\nGrouping for output\n/home/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.124305896002625 second(s)\nFiltering done in 39.780416026998864 second(s)\nNormalization done in 39.9363570860005 second(s)\nFinished making spark dataframe in 186.92885969800045 second(s)\nPredicting finished in: 5.844678331999603 second(s)\nGrouping for output\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.95381725599873 second(s)\nFiltering done in 47.53447187799975 second(s)\nNormalization done in 47.72053916100049 second(s)\nFinished making spark dataframe in 210.57170513799792 second(s)\nPredicting finished in: 5.589296361999004 second(s)\nGrouping for output\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 8.938378708000528 second(s)\nFiltering done in 40.760965688001306 second(s)\nNormalization done in 40.9408359730005 second(s)\nFinished making spark dataframe in 187.35655109699655 second(s)\nPredicting finished in: 5.575681581001845 second(s)\nGrouping for output\n/home/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.198596381000243 second(s)\nFiltering done in 45.0550046419994 second(s)\nNormalization done in 45.26876573499976 second(s)\nFinished making spark dataframe in 208.69082095900012 second(s)\nPredicting finished in: 5.086894411997491 second(s)\nGrouping for output\n/home/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.600250358998892 second(s)\nFiltering done in 42.31293970299885 second(s)\nNormalization done in 42.46333138399859 second(s)\nFinished making spark dataframe in 197.74675937800203 second(s)\nPredicting finished in: 5.596129928999289 second(s)\nGrouping for output\n/home/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.959642179001094 second(s)\nFiltering done in 46.03200198899867 second(s)\nNormalization done in 46.24938547800048 second(s)\nFinished making spark dataframe in 207.24217613499786 second(s)\nPredicting finished in: 5.098231190997467 second(s)\nGrouping for output\n/home/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\n/home/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200304_114601_SV1-02_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.450854181999603 second(s)\nFiltering done in 39.51521674400283 second(s)\nNormalization done in 39.689439378002135 second(s)\nFinished making spark dataframe in 190.84838138300256 second(s)\nPredicting finished in: 5.898512728999776 second(s)\nGrouping for output\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 10.427086648000113 second(s)\nFiltering done in 40.352837850998185 second(s)\nNormalization done in 40.53532803900089 second(s)\nFinished making spark dataframe in 189.97698182399836 second(s)\nPredicting finished in: 6.262001445997157 second(s)\nGrouping for output\n/home/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.480283937999047 second(s)\nFiltering done in 46.32159833699916 second(s)\nNormalization done in 46.45904918800079 second(s)\nFinished making spark dataframe in 208.53914567399988 second(s)\nPredicting finished in: 6.2967404990013165 second(s)\nGrouping for output\n/home/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.251330809001956 second(s)\nFiltering done in 41.017747896999936 second(s)\nNormalization done in 41.14325455500148 second(s)\nFinished making spark dataframe in 187.6864122900006 second(s)\nPredicting finished in: 6.733306923000782 second(s)\nGrouping for output\n/home/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.885407263998786 second(s)\nFiltering done in 44.833345695999014 second(s)\nNormalization done in 44.97876207399895 second(s)\nFinished making spark dataframe in 208.53120297499845 second(s)\nPredicting finished in: 6.176030564998655 second(s)\nGrouping for output\n/home/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\n/home/20190302_105829_SV1-01_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.243611224999768 second(s)\nFiltering done in 39.65820740599884 second(s)\nNormalization done in 39.78284267499839 second(s)\nFinished making spark dataframe in 187.09885762900012 second(s)\nPredicting finished in: 6.456363535999117 second(s)\nGrouping for output\n/home/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 21.586395120000816 second(s)\nFiltering done in 46.69168740599707 second(s)\nNormalization done in 46.88244692399894 second(s)\nFinished making spark dataframe in 219.9449768020022 second(s)\nPredicting finished in: 5.240294942999753 second(s)\nGrouping for output\n/home/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\n/home/20190308_111644_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.12125247899894 second(s)\nFiltering done in 40.73236160599845 second(s)\nNormalization done in 40.87371607000023 second(s)\nFinished making spark dataframe in 185.08361921899996 second(s)\nPredicting finished in: 5.65575145099865 second(s)\nGrouping for output\n/home/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.866702085000725 second(s)\nFiltering done in 43.94590059499751 second(s)\nNormalization done in 44.13988108400008 second(s)\nFinished making spark dataframe in 208.1865519239982 second(s)\nPredicting finished in: 5.21642539000095 second(s)\nGrouping for output\n/home/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20190422_111335_SV1-01_50cm_RD_11bit_RGBI_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.124305896002625 second(s)\nFiltering done in 39.780416026998864 second(s)\nNormalization done in 39.9363570860005 second(s)\nFinished making spark dataframe in 186.92885969800045 second(s)\nPredicting finished in: 5.844678331999603 second(s)\nGrouping for output\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.95381725599873 second(s)\nFiltering done in 47.53447187799975 second(s)\nNormalization done in 47.72053916100049 second(s)\nFinished making spark dataframe in 210.57170513799792 second(s)\nPredicting finished in: 5.589296361999004 second(s)\nGrouping for output\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 8.938378708000528 second(s)\nFiltering done in 40.760965688001306 second(s)\nNormalization done in 40.9408359730005 second(s)\nFinished making spark dataframe in 187.35655109699655 second(s)\nPredicting finished in: 5.575681581001845 second(s)\nGrouping for output\n/home/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.198596381000243 second(s)\nFiltering done in 45.0550046419994 second(s)\nNormalization done in 45.26876573499976 second(s)\nFinished making spark dataframe in 208.69082095900012 second(s)\nPredicting finished in: 5.086894411997491 second(s)\nGrouping for output\n/home/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\n/home/20191130_110721_SV1-01_50cm_RD_11bit_RGBI_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.600250358998892 second(s)\nFiltering done in 42.31293970299885 second(s)\nNormalization done in 42.46333138399859 second(s)\nFinished making spark dataframe in 197.74675937800203 second(s)\nPredicting finished in: 5.596129928999289 second(s)\nGrouping for output\n/home/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\nTotal permutations this step: 15411162\nDone with extracting dataframe in 20.959642179001094 second(s)\nFiltering done in 46.03200198899867 second(s)\nNormalization done in 46.24938547800048 second(s)\nFinished making spark dataframe in 207.24217613499786 second(s)\nPredicting finished in: 5.098231190997467 second(s)\nGrouping for output\n/home/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nIndex([&#39;geometry&#39;, &#39;label&#39;], dtype=&#39;object&#39;)\n/home/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_1.shp\n/home/20191202_110525_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_part_2.shp\nAppend\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20200304_114601_SV1-02_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 9.450854181999603 second(s)\nFiltering done in 39.51521674400283 second(s)\nNormalization done in 39.689439378002135 second(s)\nFinished making spark dataframe in 190.84838138300256 second(s)\nPredicting finished in: 5.898512728999776 second(s)\nGrouping for output\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sh\n\nls /dbfs/mnt/satellite-images-nso/model_out_coepelduynen"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e1a80c9-7cc3-4ddb-8b48-ba2474d819b9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height.tif_band2.save\n20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.cpg\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.dbf\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.shp\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.shx\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20210709_103835_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.cpg\n20210709_103835_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.dbf\n20210709_103835_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.shp\n20210709_103835_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.shx\n20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20220816_111150_SV2-01_SV_RD_11bit_RGBI_50cm_Scheveningen_natura2000_coepelduynen_cropped_ndvi_height.cpg\n20220816_111150_SV2-01_SV_RD_11bit_RGBI_50cm_Scheveningen_natura2000_coepelduynen_cropped_ndvi_height.dbf\n20220816_111150_SV2-01_SV_RD_11bit_RGBI_50cm_Scheveningen_natura2000_coepelduynen_cropped_ndvi_height.shp\n20220816_111150_SV2-01_SV_RD_11bit_RGBI_50cm_Scheveningen_natura2000_coepelduynen_cropped_ndvi_height.shx\nrandomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav\ntest.shp\ntest_part0.csv\ntest_part0_part_1.csv\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">20190302_105726_SV1-01_50cm_RD_11bit_RGBI_Warmond_natura2000_coepelduynen_cropped_ndvi_height.tif_band2.save\n20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20190601_105844_SV1-04_50cm_RD_11bit_RGBI_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.cpg\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.dbf\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.shp\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height.shx\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20200625_112015_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20200731_112003_SV1-03_SV_RD_11bit_RGBI_50cm_Rijnsburg_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20200915_112329_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20210709_103835_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.cpg\n20210709_103835_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.dbf\n20210709_103835_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.shp\n20210709_103835_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.shx\n20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20210815_111051_SV1-03_SV_RD_11bit_RGBI_50cm_Oegstgeest_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20210907_112017_SV1-04_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.cpg\n20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.dbf\n20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shp\n20220515_113347_SV1-02_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height_randomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav.shx\n20220816_111150_SV2-01_SV_RD_11bit_RGBI_50cm_Scheveningen_natura2000_coepelduynen_cropped_ndvi_height.cpg\n20220816_111150_SV2-01_SV_RD_11bit_RGBI_50cm_Scheveningen_natura2000_coepelduynen_cropped_ndvi_height.dbf\n20220816_111150_SV2-01_SV_RD_11bit_RGBI_50cm_Scheveningen_natura2000_coepelduynen_cropped_ndvi_height.shp\n20220816_111150_SV2-01_SV_RD_11bit_RGBI_50cm_Scheveningen_natura2000_coepelduynen_cropped_ndvi_height.shx\nrandomforest_classifier_coepelduynen_contrast_annotations_grid_search_all_data_2019_2022_small.sav\ntest.shp\ntest_part0.csv\ntest_part0_part_1.csv\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"tif_model_iterator_coepelduynen","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2718663051335708,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":1301440039569564}},"nbformat":4,"nbformat_minor":0}
