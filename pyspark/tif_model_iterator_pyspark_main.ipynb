{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c61a2717-6627-4759-9380-ae4f1b82c257","showTitle":true,"title":"TIF model iterator"}},"source":["# The TIF model iterator using pyspark\n","\n","This notebook implements a tif iterator to use a sklearn model to predict every pixel in a raster .tif file.\n","This is done in a distributed way with pyspark because of the large number of pixels and time it will take without it.\n","\n","It then outputs a segmentations shapefile which can be used in a GIS application to see the results.\n","\n","Note MinMaxScalers have to be made in order to predict because of the differences between images.\n","\n","Author: Michael de Winter"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6a485e58-c65a-4373-bfb6-989b6bec9582","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["import rasterio\n","import pandas as pd\n","import itertools\n","import numpy as np\n","import glob\n","import joblib\n","from pyspark.sql.functions import udf\n","import pickle\n","from shapely.geometry import Polygon, Point\n","from timeit import default_timer as timer\n","import geopandas as gpd\n","import os\n","from pyspark.sql.types import DoubleType, StringType, ArrayType\n","import glob\n","import shutil"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6c443219-06b2-45ba-9b92-2d8dd83e0b57","showTitle":false,"title":""}},"source":["###### run the below command to import all the required functions"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"208477fb-2595-4bb4-a50e-97265b49951c","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["%run ./tif_model_iterator_functions"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0672bd15-3eda-4a7b-88ca-f068b1005a61","showTitle":false,"title":""}},"source":["###Run settings"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f8319118-abc8-4061-b5e6-5eba1833eaa1","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Coepelduynen\n","path_to_output = \"/dbfs/mnt/PATH/TO/OUTPUT\"\n","path_to_scalers = \"/dbfs/mnt/PATH/TO/SCALERS\"\n","parts = 2\n","model_path = \"/dbfs/mnt/PATH/TO/MODEL\" "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1ef07dd1-51dc-494c-bbc5-34ed75988207","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">20190302 already done\n","20190302 already done\n","20190308 already done\n","20190422 already done\n","20190601 already done\n","20191130 already done\n","20191202 already done\n","20200304 already done\n","20200323 already done\n","20200326 already done\n","20200508 already done\n","20200625 already done\n","20200731 already done\n","20200915 already done\n","20201231 already done\n","20210302 already done\n","20210423 already done\n","20210709 already done\n","20210815 already done\n","20210907 already done\n","/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20211226_103526_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n","/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","Total permutations this step: 15415981\n","Done with extracting dataframe in 8.513373872000102 second(s)\n","Filtering done in 38.381067487999644 second(s)\n","Filtered length of dataframe: 3535439\n","Normalization done in 38.55129070999965 second(s)\n","Finished making spark dataframe in 195.596199695 second(s)\n","Predicting finished in: 0.13339372999962507 second(s)\n","/databricks/spark/python/pyspark/sql/pandas/conversion.py:161: UserWarning: toPandas attempted Arrow optimization because &#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has reached the error below and can not continue. Note that &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; does not have an effect on failures in the middle of computation.\n","  An error occurred while calling o1335.getResult.\n",": org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n","\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n","\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n","\tat py4j.Gateway.invoke(Gateway.java:295)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n","\tat java.lang.Thread.run(Thread.java:748)\n","Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 0.0 failed 4 times, most recent failure: Lost task 20.3 in stage 0.0 (TID 55) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\n","Traceback (most recent call last):\n","  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\n","NameError: name &#39;loaded_model&#39; is not defined\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n","\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n","\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n","\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n","\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n","\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n","\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n","\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\n","Caused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\n","Traceback (most recent call last):\n","  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\n","NameError: name &#39;loaded_model&#39; is not defined\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n","\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n","\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n","\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","  warnings.warn(msg)\n","An error occurred while calling o1335.getResult.\n",": org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n","\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n","\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n","\tat py4j.Gateway.invoke(Gateway.java:295)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n","\tat java.lang.Thread.run(Thread.java:748)\n","Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 0.0 failed 4 times, most recent failure: Lost task 20.3 in stage 0.0 (TID 55) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\n","Traceback (most recent call last):\n","  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\n","NameError: name &#39;loaded_model&#39; is not defined\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n","\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n","\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n","\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n","\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n","\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n","\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n","\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\n","Caused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\n","Traceback (most recent call last):\n","  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\n","NameError: name &#39;loaded_model&#39; is not defined\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\n","*** WARNING: skipped 88767 bytes of output ***\n","\n","  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\n","NameError: name &#39;loaded_model&#39; is not defined\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n","\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n","\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n","\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n","\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n","\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n","\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n","\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\n","Caused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\n","Traceback (most recent call last):\n","  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\n","NameError: name &#39;loaded_model&#39; is not defined\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n","\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n","\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n","\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","  warnings.warn(msg)\n","An error occurred while calling o3634.getResult.\n",": org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n","\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n","\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n","\tat py4j.Gateway.invoke(Gateway.java:295)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n","\tat java.lang.Thread.run(Thread.java:748)\n","Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 4.0 failed 4 times, most recent failure: Lost task 5.3 in stage 4.0 (TID 357) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\n","Traceback (most recent call last):\n","  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\n","NameError: name &#39;loaded_model&#39; is not defined\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n","\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n","\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n","\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n","\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n","\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n","\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n","\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n","\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\n","Caused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\n","Traceback (most recent call last):\n","  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\n","NameError: name &#39;loaded_model&#39; is not defined\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n","\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n","\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n","\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n","\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","20220922 already done\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">20190302 already done\n20190302 already done\n20190308 already done\n20190422 already done\n20190601 already done\n20191130 already done\n20191202 already done\n20200304 already done\n20200323 already done\n20200326 already done\n20200508 already done\n20200625 already done\n20200731 already done\n20200915 already done\n20201231 already done\n20210302 already done\n20210423 already done\n20210709 already done\n20210815 already done\n20210907 already done\n/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/20211226_103526_SV1-01_SV_RD_11bit_RGBI_50cm_KatwijkAanZee_natura2000_coepelduynen_cropped_ndvi_height.tif\n/databricks/python/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nTotal permutations this step: 15415981\nDone with extracting dataframe in 8.513373872000102 second(s)\nFiltering done in 38.381067487999644 second(s)\nFiltered length of dataframe: 3535439\nNormalization done in 38.55129070999965 second(s)\nFinished making spark dataframe in 195.596199695 second(s)\nPredicting finished in: 0.13339372999962507 second(s)\n/databricks/spark/python/pyspark/sql/pandas/conversion.py:161: UserWarning: toPandas attempted Arrow optimization because &#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has reached the error below and can not continue. Note that &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; does not have an effect on failures in the middle of computation.\n  An error occurred while calling o1335.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 0.0 failed 4 times, most recent failure: Lost task 20.3 in stage 0.0 (TID 55) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n  warnings.warn(msg)\nAn error occurred while calling o1335.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 0.0 failed 4 times, most recent failure: Lost task 20.3 in stage 0.0 (TID 55) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\n*** WARNING: skipped 88767 bytes of output ***\n\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n  warnings.warn(msg)\nAn error occurred while calling o3634.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 4.0 failed 4 times, most recent failure: Lost task 5.3 in stage 4.0 (TID 357) (10.0.4.6 executor 0): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2752)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3887)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3891)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3857)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3856)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;loaded_model&#39; is not defined&#39;, from &lt;command-206828270476792&gt;, line 135. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-206828270476792&gt;&#34;, line 135, in predict_pandas_udf\nNameError: name &#39;loaded_model&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:206)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$7(Dataset.scala:3889)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2751)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n20220922 already done\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Predict all .tif files with a model.\n","for file  in glob.glob(\"/dbfs/mnt/satellite-images-nso/SV_50cm/coepelduynen/*.tif\"):\n","  if file.split(\"/\")[-1].split(\"_\")[0] not in set(check_done_files()):\n","    try:\n","      print(file)\n","      run_tif_model_implementer(file,path_to_output,path_to_scalers,parts,model_path, aggregate_to_2m = False)\n","    except Exception as e:\n","      print(e)\n","  else:\n","      print(file.split(\"/\")[-1].split(\"_\")[0]+\" already done\")"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2718663051335708,"dataframes":["_sqldf"]},"pythonIndentUnit":2},"notebookName":"tif_model_iterator_coepelduynen","notebookOrigID":1301440039569564,"widgets":{}},"kernelspec":{"display_name":"Python 3.7.3 ('base')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.3"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}}},"nbformat":4,"nbformat_minor":0}
